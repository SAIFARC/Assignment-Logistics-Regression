{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "- Simple Linear Regression is a statistical method that models the relationship between one independent variable (x) and one dependent variable (y) using a straight line. The purpose is to predict the value of y from x and to understand how changes in x are associated with changes in y.\n",
        "It’s simple and easy to interpret, so people use it to get a first idea of association and for basic predictions.\n"
      ],
      "metadata": {
        "id": "YmHbSKW1_wGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "- The main assumptions are:\n",
        "Linearity: the relationship between x and y is linear (a straight line).\n",
        "Independence: the observations (errors) are independent of each other.\n",
        "Homoscedasticity: the errors have constant variance across all values of x.\n",
        "Normality of errors: the residuals (errors) are approximately normally distributed (important for inference).\n",
        "Correct model specification: no important variable or non-linear term is missing.\n",
        "For SLR specifically, multicollinearity is not relevant because there is only one predictor.\n"
      ],
      "metadata": {
        "id": "yfiJ6_q5_y8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "- The equation is: y = β0 + β1*x + ε\n",
        "Here, y is the dependent (response) variable we want to predict.\n",
        "β0 is the intercept (value of y when x = 0).\n",
        "β1 is the slope (change in y for a one unit change in x).\n",
        "x is the independent (predictor) variable.\n",
        "ε (epsilon) is the error term capturing the difference between observed and predicted y.\n"
      ],
      "metadata": {
        "id": "sRtYCkpaABd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Provide a real-world example where simple linear regression can be applied.\n",
        "- Example: Predicting house price from house size. If we have data on house area (in square feet) and house sale price, SLR can estimate how much price increases per additional square foot and can be used to predict price for a given area.\n",
        "Other simple examples: predicting student test score from hours studied, or predicting fuel consumption from vehicle weight.\n"
      ],
      "metadata": {
        "id": "XrfZAD_WAC06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the method of least squares in linear regression?\n",
        "- The method of least squares finds the line (β0 and β1) that minimizes the sum of squared differences between observed y values and the values predicted by the line (residuals). In practice we compute residuals (yi - ŷi), square them, sum over all observations, and choose parameters that make this sum as small as possible. This gives the best linear unbiased estimates under the usual assumptions.\n"
      ],
      "metadata": {
        "id": "3OlX-dQTAE7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "- Logistic Regression is used for classification (usually binary). Instead of predicting a continuous y, it models the probability that y belongs to a class using the logistic (sigmoid) function on a linear combination of predictors.\n",
        "The key differences: linear regression predicts continuous outcomes; logistic regression predicts probabilities (and class labels). Logistic uses log-odds and a sigmoid link, and it is trained with a likelihood (cross-entropy) loss instead of least squares.\n"
      ],
      "metadata": {
        "id": "pC9SNY7yAGdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Name and briefly describe three common evaluation metrics for regression models.\n",
        "- Mean Squared Error (MSE): average of squared differences between actual and predicted values. Penalizes large errors more.\n",
        "Root Mean Squared Error (RMSE): square root of MSE, in the same units as the target, easier to interpret.\n",
        "Mean Absolute Error (MAE): average of absolute differences between actual and predicted values; more robust to outliers than MSE.\n"
      ],
      "metadata": {
        "id": "nUJ83wFuAI8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the purpose of the R-squared metric in regression analysis?\n",
        "- R-squared measures the proportion of variance in the dependent variable that is explained by the model. It ranges from 0 to 1 (sometimes reported as a percentage). A higher R-squared means the model explains more of the variability in the outcome. It’s useful for comparing models but should be used along with other diagnostics (it doesn’t prove causation and can increase with unnecessary predictors).\n"
      ],
      "metadata": {
        "id": "Nqfgs90iAR9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple linear regression model?\n",
        "- The slope (β1) tells how much the average value of y changes when x increases by one unit, holding everything else constant. For example, a slope of 0.6 means y increases by 0.6 for each 1-unit increase in x on average.\n",
        "The intercept (β0) is the predicted value of y when x = 0. Interpretation of the intercept is only meaningful if x = 0 is within the range of observed data; otherwise treat it with caution.\n",
        "Also remember to check statistical significance, confidence intervals, and model assumptions before drawing strong conclusions from coefficients.\n"
      ],
      "metadata": {
        "id": "XfBHRHgcUNPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "W4XTetvhToJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Slope:\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMfE1Xs5TryP",
        "outputId": "23553fc1-5392-4afd-f618-51da5dd7019b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope: 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    }
  ]
}